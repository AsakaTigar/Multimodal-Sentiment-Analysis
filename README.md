# Multimodal-Sentiment-Analysis
多模态情感分析（Multimodal Sentiment Analysis，MSA）是指通过融合来自不同模态（如文本、视觉、音频等）的信息来分析和预测情感的任务。在情感分析任务中，通常我们只处理文本数据，但随着计算能力和深度学习技术的发展，利用多种模态信息进行情感识别的研究越来越受到关注。不同模态提供的情感信息可以互相补充，从而提高分析的准确性和鲁棒性。
原文：
![](Pasted%20image%2020241019151141.png)
![](Pasted%20image%2020241019151856.png)
CMKA 模块可以用 LSTM 模块进行补充

CMU-MOSI和 CMU-MOSEI

CMU-MOSI：文本、语音、视觉、情感标注
CMU-MOSEI：更多情感标注和情绪标注，包括高兴、悲伤、生气等六种情绪
RAVDESS：三种：纯音频、音频-视频、纯视频

情绪的三种表征形式；数据的采集，转录和注释
增加情绪表征：六种埃克曼情绪？极性取向

多模态特征编码器的最优选择和融合，并将这些特征编码器组合在一个神经网络中以改进情感检测。
论文：# Multimodal Multi-loss Fusion Network for Sentiment Analysis情感分析的多模态多损失融合网络
代码：[zehuiwu/MMML: Multi-Modality Multi-Loss Fusion Network](https://github.com/zehuiwu/MMML)
![](Pasted%20image%2020241120211136.png)
- **左图（Figure 1）**：标准 MMIM 模型，融合文本和音频特征后通过全连接层预测输出。
- **右图（Figure 2）**：另一种变体，使用不同的连接方式（Concatenation/Transformer）以优化信息流。
主要模块包括：
- **Cross-Attention Encoder**：实现模态间的==交叉注意力==机制，帮助模态间信息交互。
- **Self-Attention Encoder**：对单模态数据进行进一步的内部特征提取。
- **Pointwise Feed-Forward Network**：进一步增强提取的表示。
- **全连接层**：进行最终的分类或回归预测。

1. **提出分层互信息最大化框架**：
    - 同时在输入和融合级别最大化互信息。
    - 第一次将==互信息理论引入==多模态情感分析任务。
2. **高效的互信息估计方法**：
    - 提出了结合参数化（基于神经网络）和非参数化（基于高斯混合模型，GMM）的混合方法，解决互信息直接估计的难题。
3. **性能提升**：
    - 在两个基准数据集（CMU-MOSI 和 CMU-MOSEI）上展示了优于当前最先进模型的结果，特别是在情感预测准确性上有显著改进。
![](Pasted%20image%2020241120211945.png)
![](Pasted%20image%2020241120212035.png)
论文：任务内和任务间动态的多模态情感分析的判别联合多任务框架
![](Pasted%20image%2020241120213809.png)
### **. 输入模块**

- **Acoustic Encoder (音频编码器)**：处理音频信号，提取特征 $XaX_aXa$​。
- **Text Encoder (文本编码器)**：对文本序列进行编码，提取文本特征 $XtX_tXt​$。
- **Visual Encoder (视觉编码器)**：对视频帧进行编码，提取视觉特征 $XvX_vXv$​。
### **Dual Bidirectional Encoder (DBE，双向编码器)**

这个模块的核心是多个交互块（blocks）用于跨模态信息交互：

- **TA Block（Text-Acoustic 交互块）**：实现文本和音频之间的特征交互。
- **AT Block（Acoustic-Text 交互块）**：实现音频到文本方向的特征交互。
- **TV Block（Text-Visual 交互块）**：实现文本和视觉之间的特征交互。
- **VT Block（Visual-Text 交互块）**：实现视觉到文本方向的特征交互。

这些交互块按层次堆叠 ($xLxLxL$ 层)，使不同模态之间的信息能够双向流动，增强多模态特征融合的表现力。
### **特征金字塔网络 (Feature Pyramid Networks, FPNs)**

- **结构**：
    
    - 包括底层到高层的特征提取（Bottom-up）和==高层到底层的反馈机制==（Top-down）。
    - 多层特征表示通过最大池化（max pooling）操作逐层提取，形成不同层次的信息表示。
- **作用**：
    
    - **任务内动态学习（Intra-task Dynamics）**：通过分层提取任务相关的多尺度特征。
    - **任务间联合学习（Inter-task Dynamics）**：通过不同任务共享金字塔特征表示，实现任务间的协作优化。
### **关键技术亮点**

1. **联合训练策略**：
    
    - 实现任务间==动态协作==优化（蓝色框中策略）。
    - 任务间信息交互通过金字塔网络共享多层次特征。
2. **区分性学习策略**：
    
    - 在每个任务中通过金字塔网络逐层提取==细化特征==，解决任务内动态问题（红色框中策略）
![](Pasted%20image%2020241120214254.png)
**核心组件**：

- **CMA (Cross-Modal Attention)**：
    - 使用查询 $(Q)、键 (K) 和值 (V)$ 构造跨模态注意力机制，融合来自文本和声学的特征。
    - CMA输出 $YtaY_{ta}Yta​$，表示融合后的特征。
- **MIM估计器**：
    - MIM (Mutual Information Maximization) 估计器通过==最大化输入模态==间的互信息来优化表示，提升跨模态特征的相关性。
- **Add&Norm 和 FFN**：
    - Add&Norm执行残差连接与归一化。
    - FFN (前馈网络) 用于进一步处理特征。
![](Pasted%20image%2020241120214915.png)
论文 ：基于注意机制的多模态融合网络视觉文本情感分析
![](Pasted%20image%2020241121114615.png)
### **. 特征提取**

- **文本特征提取**：
    - 使用双向LSTM（BiLSTM）提取序列特征。
    - 通过自注意力机制（Self-Attention）增强特征表示。
- **图像特征提取**：
    - 应用多头注意力（Multi-head Attention）提取图像中与文本对齐的上下文信息。
### **. 多模态对齐**

- 将图像和文本特征通过加法或拼接进行初步融合，实现初步的多模态对齐。
### **. 多模态细粒度融合**
- 通过多模态相关性获取模块（Multi-modal Correlation Acquisition）计算细粒度的跨模态相关性矩阵。
- 处理并拼接相关性特征，形成更高级的联合表示。
### **. 分类**
- 联合表示经过全连接层（FC layers）进一步处理。
- 使用Softmax分类器输出情感类别（如正向/负向）。
![](Pasted%20image%2020241121145504.png)
**非线性激活与融合**：

- $C_P$ 和 $C_T$分别通过非线性激活函数（Tanh）处理，生成增强后的特征。
- 融合特征与原始输入特征通过元素级加法相结合，生成最终的输出特征 P′和 T′。
![](Pasted%20image%2020241121150029.png)
![](Pasted%20image%2020241121150039.png)
论文：基于多任务学习的模态一致性和差异信息的多模态情感分析
![](Pasted%20image%2020241121151350.png)
没啥改进的，重点依旧是融合对。动态权重分配 (Attention Network),跨模态Transformer捕捉上下文关系,联合表示翻译模块处理模态对关系,
![](Pasted%20image%2020241121151730.png)
![](Pasted%20image%2020241121152721.png)
- **Multi-Head Attention (MI(BI, UI))**：多头注意力机制用于跨模态的信息交互。
- **Add & Norm**：在多头注意力之后添加残差连接，并进行归一化操作。
- **Feed Forward**：通过前馈神经网络进一步处理每个位置的表示。
- **重复N次**：整个Transformer块通常会重复N次来增强表达能力。
==跨模态Transformer模块==
![](Pasted%20image%2020241121152852.png)
![](Pasted%20image%2020241121152950.png)

论文：基于自适应特征归一化增强多模态交互的多模态情感分析
![](Pasted%20image%2020241121153242.png)
- **AdaFN（自适应融合网络）**：
    - AdaFN模块用于处理音频和视觉特征。这些特征首先经过自适应融合网络（AdaFN），旨在适应性地融合模态特征，使其更适合后续情感分析任务。
- **sLSTM（可变状态长短时记忆网络）**：
    - sLSTM模块用于捕获输入模态（音频和视觉）之间的时序关系，帮助捕获情感动态。它的设计灵活，可以处理可变长度的输入数据。
- **Att Gate（注意力门控机制）**：
    - 注意力机制用于选择最相关的信息，从每个模态中提取情感相关特征。它可以提升模型对输入特征的选择性，从而更好地捕获情感信息。
- **特征融合（Fm）**：
    - 最终的多模态特征（Fm）是通过将文本、音频和视觉特征融合而成，融合方式通过不同模态的拼接来实现，以便结合每个模态的特征。
- **线性层和ReLU激活**：
    - 特征融合后通过线性层和ReLU激活函数进行特征映射，得到输出特征（如图中 **F*m** 和 **F*s** 所示），进一步用于预测。
- **ULGM（联合学习生成模块）**：
    - ULGM模块是联合学习的一部分，它在融合后的特征上训练以生成最终的预测。目标是通过各模态的联合特征来提升预测的准确性。、
注意力门控机制（Attention Gating Mechanism）
![](Pasted%20image%2020241121153452.png)
- **BiGRU（双向门控循环单元）**：
    - 对文本和目标模态的输入分别使用 **BiGRU** 进行处理，目的是捕捉输入数据的上下文信息。双向GRU有助于利用历史和未来信息，从而更好地理解情感特征。
    - **F_text** 和 **F_target** 分别是经过 BiGRU 处理后的文本和目标模态的特征表示。
- **Self-Attention（自注意力）**：
    - **Self-Attn** 模块用于计算输入内部的自相关性，这样模型可以关注到输入自身的重要部分，从而更好地表示每个模态的情感特征。
- **Cross-Attention（交叉注意力）**：
    - **Cross-Attn** 模块用于捕捉文本与目标模态之间的相互关系。通过交叉注意力，文本和目标模态的相关信息可以相互融合，从而增强对情感的联合理解。
- **Add & Norm（加和与归一化）**：
    - **Add & Norm** 模块在 **Self-Attn** 和 **Cross-Attn** 之后应用，用于进行残差连接并对特征进行归一化，以保持网络的稳定性和梯度传递的有效性。
- **Gate Mechanism（门控机制）**：
    - **Gate1 和 Gate2** 模块用于选择性地通过或抑制特定的信息。门控机制可以控制不同模态的特征对最终输出的影响，从而增强模型的情感预测能力。
- **Ave-pooling（平均池化）**：
    - **Ave-pooling** 模块用于对特征进行降维和聚合，将信息压缩到一个较小的维度，以减少计算量并为后续处理做准备。
![](Pasted%20image%2020241121160305.png)

论文：不确定缺失模态的多模态情感分析统一自蒸馏框架
![](Pasted%20image%2020241121161308.png)
- **输入模态**：
    
    - **语言（Language）**、**音频（Audio）** 和 **视觉（Visual）**：分别从文本、音频和视频中提取相应特征。
- **MCIM（Multi-modal Coarse and Fine-grained Interaction Module，多模态粗细粒度交互模块）**：
    - **粗粒度注意力（Coarse-grained CA）** 和 **细粒度注意力（Fine-grained CA）**：
        - **粗粒度注意力** 负责捕捉全局信息，从整体上理解多模态之间的关系。
        - **细粒度注意力** 更注重细节，对局部的特征进行更精细的交互分析。
    - **Transformer 编码器**：用于处理和编码融合后的特征。
    - **特征表示 $H_a$ 和 $H_b$**：表示不同阶段的多模态融合结果。
- **分类器和共享训练**：
    
    - 特征表示 $H_a$和 $H_b$ 通过各自的分类器进行情感分类或回归预测。
    - **共享部分**：分类器之间有共享的特征学习部分，目的是通过不同模态之间的互相学习，提升情感分析的泛化能力。
    - **损失函数（$L_fea$ 和 $L_logits$）**：
        - $L_fea$：用于特征层的共享损失，通过对齐特征来提高不同模态之间的互补性。
        - $L_logits$：用于分类器输出的损失，确保预测的一致性。
- **DFIM（Frame-level Self-enhancement and Selective Filtering Module，帧级自增强和选择性过滤模块）**：
    
    - **帧级自增强（Frame-level Self-enhancement）**：用于增强帧级别的特征，使得重要的帧信息得到更多的关注。
    - **选择性过滤（Selective Filtering）**：过滤掉不相关的帧级特征，保留最能体现情感的部分，进一步提高模型的准确性和鲁棒性。
    - 最终得到的融合特征 **Ĥ**，用于推断阶段的情感预测。
- **推断与任务损失（$L_task$）**：
    
    - $L_task$：用于最终预测任务的损失函数，可以是分类或者回归任务，用来指导模型学习最适合的情感特征表示
![](Pasted%20image%2020241121162143.png)
`CrossAttention` 模块实现了不同模态特征之间的交互，通过多头注意力机制来计算各个模态之间的相关性。使用 `nn.MultiheadAttention` 来实现交叉模态之间的特征融合，随后通过 `LayerNorm` 进行归一化，确保模型在特征学习过程中保持稳定。

`UpdatedMCIM` 模块主要用于粗细粒度的模态交互，通过多个交叉注意力模块，分别在不同模态对之间执行特征交互，然后将交互的特征进行融合并通过前馈神经网络（FFN）增强特征。

`UpdatedMultimodalSentimentAnalysisModel` 是整个多模态情感分析的主模型，包含以下步骤：
- **MCIM 处理**：首先对语言、音频、视觉模态进行粗细粒度的交互处理，得到融合的特征。
- **DFIM 处理**：然后对融合特征进行帧级自增强和选择性过滤，进一步优化特征表达。
- **分类器**：最后使用分类器对优化后的特征进行情感预测，输出最终的分类或回归结果。

![](Pasted%20image%2020241121162933.png)

论文：Learning Language-guided Adaptive Hyper-modality Representation for
Multimodal Sentiment Analysis![](Pasted%20image%2020241126172941.png)
![](Pasted%20image%2020241126173356.png)



Vit 差分 Transformer（配置文本和语音的权重） 的运用，==特征学习==
==特征融合==和提取，多模态
循环差分 Transformer
