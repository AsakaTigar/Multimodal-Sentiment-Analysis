class AutoFusionCompression(nn.Module):
    """自适应上下文压缩模块"""
    def __init__(self, dim: int, num_modalities: int = 3, reduction_ratio: int = 4):
        super().__init__()
        self.context_gate = nn.Sequential(
            nn.Linear(dim*num_modalities, dim//reduction_ratio),
            nn.ReLU(inplace=True),
            nn.Linear(dim//reduction_ratio, num_modalities),
            nn.Softmax(dim=-1)
        )
        self.cross_modal_attn = nn.MultiheadAttention(dim, num_heads=8, batch_first=True)
        
    def forward(self, modalities: list[torch.Tensor]) -> torch.Tensor:
        # 模态间交叉注意力（保留上下文关联）
        compressed = []
        for mod in modalities:
            out, _ = self.cross_modal_attn(mod, torch.stack(modalities).mean(0), mod)
            compressed.append(out)
        
        # 自适应门控融合（ICML 2025）
        gate_weights = self.context_gate(torch.cat(modalities, dim=-1))
        return torch.stack([w*m for w, m in zip(gate_weights.unbind(-1), compressed)]).sum(dim=0)

class GANFusionRegularizer(nn.Module):
    """基于Wasserstein GAN的潜在空间正则化模块"""
    def __init__(self, latent_dim: int, num_modalities: int = 3):
        super().__init__()
        # 生成器（多模态互补信息融合）
        self.generator = nn.Sequential(
            nn.Linear(latent_dim*num_modalities, latent_dim*2),
            nn.LayerNorm(latent_dim*2),
            nn.GELU(),
            nn.Linear(latent_dim*2, latent_dim)
        )
        
        # 判别器（模态一致性验证）
        self.discriminator = nn.Sequential(
            nn.Linear(latent_dim, latent_dim//2),
            nn.LeakyReLU(0.2),
            nn.Linear(latent_dim//2, 1)
        )
        
        # 梯度惩罚系数（WGAN-GP）
        self.gp_weight = 0.1
        
    def forward(self, z: torch.Tensor, real_data: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        # 生成融合特征
        fake_data = self.generator(z)
        
        # 判别器损失计算
        real_loss = -self.discriminator(real_data.detach()).mean()
        fake_loss = self.discriminator(fake_data.detach()).mean()
        gp_loss = self._gradient_penalty(real_data, fake_data)
        
        # 生成器对抗损失
        adv_loss = -self.discriminator(fake_data).mean()
        
        return fake_data, real_loss + fake_loss + self.gp_weight*gp_loss, adv_loss
    
    def _gradient_penalty(self, real: torch.Tensor, fake: torch.Tensor) -> torch.Tensor:
        """WGAN-GP梯度惩罚项"""
        batch_size = real.size(0)
        alpha = torch.rand(batch_size, 1, device=real.device)
        interpolates = alpha*real + (1-alpha)*fake
        interpolates.requires_grad_(True)
        
        d_interpolates = self.discriminator(interpolates)
        gradients = torch.autograd.grad(
            outputs=d_interpolates,
            inputs=interpolates,
            grad_outputs=torch.ones_like(d_interpolates),
            create_graph=True,
            retain_graph=True,
            only_inputs=True
        )[0]
        return ((gradients.norm(2, dim=1) - 1) ** 2).mean()

class EnhancedFusionLayer(nn.Module):
    """集成AutoFusion与GAN-Fusion的增强模块"""
    def __init__(self, dim: int, num_modalities: int = 3):
        super().__init__()
        self.autofusion = AutoFusionCompression(dim, num_modalities)
        self.gan_fusion = GANFusionRegularizer(dim)
        
        # 残差连接（防止信息丢失）
        self.res_conv = nn.Conv1d(dim, dim, kernel_size=3, padding=1)
        
        # 动态权重学习（参考DeepSeek最新架构）
        self.learned_weights = nn.Parameter(torch.ones(2))
        
    def forward(self, modalities: list[torch.Tensor]) -> torch.Tensor:
        # AutoFusion压缩
        compressed = self.autofusion(modalities)
        
        # GAN正则化
        z = torch.cat(modalities, dim=-1).detach()
        fused, d_loss, g_loss = self.gan_fusion(z, compressed)
        
        # 残差连接与动态融合
        res_out = self.res_conv(compressed.permute(0,2,1)).permute(0,2,1)
        weights = torch.softmax(self.learned_weights, dim=0)
        
        return weights[0]*fused + weights[1]*res_out, d_loss, g_loss
